{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP CW 2 - PART B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T22:21:31.437736Z",
     "start_time": "2017-11-21T22:21:31.128464Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T22:21:31.975229Z",
     "start_time": "2017-11-21T22:21:31.486888Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "def train_model_and_plot_stats(model,\n",
    "                               error,\n",
    "                               learning_rule,\n",
    "                               train_data,\n",
    "                               valid_data,\n",
    "                               test_data,\n",
    "                               num_epochs,\n",
    "                               stats_interval,\n",
    "                               notebook=False,\n",
    "                               earlyStopping=True):\n",
    "\n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors = {'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    if earlyStopping:\n",
    "        optimiser = EarlyStoppingOptimiser(\n",
    "            model,\n",
    "            error,\n",
    "            learning_rule,\n",
    "            train_data,\n",
    "            valid_data,\n",
    "            test_data,\n",
    "            data_monitors,\n",
    "            notebook=notebook,\n",
    "            steps=3,\n",
    "            patience=3)\n",
    "        stats, key, run_time, best_epoch = optimiser.train(\n",
    "            max_num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "        return (stats, key, run_time, best_epoch)\n",
    "    else:\n",
    "        optimiser = Optimiser(\n",
    "            model,\n",
    "            error,\n",
    "            learning_rule,\n",
    "            train_data,\n",
    "            valid_data,\n",
    "            test_data,\n",
    "            data_monitors,\n",
    "            notebook=notebook)\n",
    "\n",
    "        stats, keys, run_time = optimiser.train(\n",
    "            num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "        return (stats, key, run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T22:21:43.804348Z",
     "start_time": "2017-11-21T22:21:43.150405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'targets']\n",
      "['inputs', 'targets']\n",
      "['inputs', 'targets']\n"
     ]
    }
   ],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016\n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "test_data = EMNISTDataProvider('test', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T22:41:54.570449Z",
     "start_time": "2017-11-21T22:41:54.519620Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "from mlp.layers import *\n",
    "from mlp.errors import CrossEntropyLogSoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule, AdamLearningRule, RMSPropLearningRule\n",
    "from mlp.optimisers import Optimiser, EarlyStoppingOptimiser\n",
    "\n",
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T00:13:11.889059Z",
     "start_time": "2017-11-21T22:41:55.000451Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 180.5s to complete\n",
      "    error(train)=3.85e+00, acc(train)=1.65e-02, error(valid)=3.85e+00, acc(valid)=1.78e-02, error(test)=3.85e+00, acc(test)=1.69e-02\n",
      "Epoch 2: 180.8s to complete\n",
      "    error(train)=7.89e-01, acc(train)=7.54e-01, error(valid)=8.13e-01, acc(valid)=7.44e-01, error(test)=8.38e-01, acc(test)=7.41e-01\n",
      "Epoch 3: 180.9s to complete\n",
      "    error(train)=6.11e-01, acc(train)=8.06e-01, error(valid)=6.42e-01, acc(valid)=7.97e-01, error(test)=6.72e-01, acc(test)=7.88e-01\n",
      "Epoch 4: 180.7s to complete\n",
      "    error(train)=5.15e-01, acc(train)=8.30e-01, error(valid)=5.57e-01, acc(valid)=8.18e-01, error(test)=5.90e-01, acc(test)=8.08e-01\n",
      "Epoch 5: 181.0s to complete\n",
      "    error(train)=4.72e-01, acc(train)=8.43e-01, error(valid)=5.23e-01, acc(valid)=8.28e-01, error(test)=5.55e-01, acc(test)=8.21e-01\n",
      "Epoch 6: 185.4s to complete\n",
      "    error(train)=4.40e-01, acc(train)=8.52e-01, error(valid)=5.05e-01, acc(valid)=8.32e-01, error(test)=5.37e-01, acc(test)=8.24e-01\n",
      "Epoch 7: 183.6s to complete\n",
      "    error(train)=4.29e-01, acc(train)=8.51e-01, error(valid)=5.02e-01, acc(valid)=8.31e-01, error(test)=5.35e-01, acc(test)=8.24e-01\n",
      "Epoch 8: 181.5s to complete\n",
      "    error(train)=4.08e-01, acc(train)=8.55e-01, error(valid)=4.92e-01, acc(valid)=8.33e-01, error(test)=5.28e-01, acc(test)=8.22e-01\n",
      "Epoch 9: 185.3s to complete\n",
      "    error(train)=3.91e-01, acc(train)=8.60e-01, error(valid)=4.82e-01, acc(valid)=8.33e-01, error(test)=5.25e-01, acc(test)=8.22e-01\n",
      "Epoch 10: 180.4s to complete\n",
      "    error(train)=3.53e-01, acc(train)=8.74e-01, error(valid)=4.61e-01, acc(valid)=8.46e-01, error(test)=4.96e-01, acc(test)=8.35e-01\n",
      "Epoch 11: 181.3s to complete\n",
      "    error(train)=3.33e-01, acc(train)=8.82e-01, error(valid)=4.51e-01, acc(valid)=8.48e-01, error(test)=4.82e-01, acc(test)=8.40e-01\n",
      "Epoch 12: 180.6s to complete\n",
      "    error(train)=3.26e-01, acc(train)=8.80e-01, error(valid)=4.49e-01, acc(valid)=8.47e-01, error(test)=4.90e-01, acc(test)=8.39e-01\n",
      "Epoch 13: 181.7s to complete\n",
      "    error(train)=3.15e-01, acc(train)=8.86e-01, error(valid)=4.53e-01, acc(valid)=8.50e-01, error(test)=4.94e-01, acc(test)=8.38e-01\n",
      "Epoch 14: 181.1s to complete\n",
      "    error(train)=3.07e-01, acc(train)=8.87e-01, error(valid)=4.58e-01, acc(valid)=8.46e-01, error(test)=4.95e-01, acc(test)=8.39e-01\n",
      "Epoch 15: 181.0s to complete\n",
      "    error(train)=3.10e-01, acc(train)=8.84e-01, error(valid)=4.68e-01, acc(valid)=8.43e-01, error(test)=5.11e-01, acc(test)=8.36e-01\n",
      "Epoch 16: 181.6s to complete\n",
      "    error(train)=2.81e-01, acc(train)=8.96e-01, error(valid)=4.44e-01, acc(valid)=8.54e-01, error(test)=4.87e-01, acc(test)=8.43e-01\n",
      "UP1: error(valid) at 16 = 4.65e-01 > at 11 = 4.49e-01\n",
      "Epoch 17: 181.1s to complete\n",
      "    error(train)=2.83e-01, acc(train)=8.95e-01, error(valid)=4.65e-01, acc(valid)=8.49e-01, error(test)=5.05e-01, acc(test)=8.39e-01\n",
      "UP1: error(valid) at 17 = 4.71e-01 > at 12 = 4.53e-01\n",
      "Epoch 18: 181.1s to complete\n",
      "    error(train)=2.82e-01, acc(train)=8.93e-01, error(valid)=4.71e-01, acc(valid)=8.47e-01, error(test)=5.09e-01, acc(test)=8.39e-01\n",
      "Epoch 19: 181.2s to complete\n",
      "    error(train)=2.60e-01, acc(train)=9.04e-01, error(valid)=4.56e-01, acc(valid)=8.54e-01, error(test)=5.01e-01, acc(test)=8.44e-01\n",
      "Epoch 20: 180.7s to complete\n",
      "    error(train)=2.51e-01, acc(train)=9.07e-01, error(valid)=4.64e-01, acc(valid)=8.55e-01, error(test)=5.08e-01, acc(test)=8.47e-01\n",
      "UP1: error(valid) at 20 = 4.71e-01 > at 15 = 4.44e-01\n",
      "Epoch 21: 181.0s to complete\n",
      "    error(train)=2.50e-01, acc(train)=9.06e-01, error(valid)=4.71e-01, acc(valid)=8.52e-01, error(test)=5.22e-01, acc(test)=8.37e-01\n",
      "UP1: error(valid) at 21 = 4.93e-01 > at 16 = 4.65e-01\n",
      "UP2: error(valid) at 16 = 4.65e-01 > at 11 = 4.49e-01\n",
      "Epoch 22: 180.9s to complete\n",
      "    error(train)=2.51e-01, acc(train)=9.03e-01, error(valid)=4.93e-01, acc(valid)=8.46e-01, error(test)=5.28e-01, acc(test)=8.39e-01\n",
      "UP1: error(valid) at 22 = 4.78e-01 > at 17 = 4.71e-01\n",
      "UP2: error(valid) at 17 = 4.71e-01 > at 12 = 4.53e-01\n",
      "Epoch 23: 180.9s to complete\n",
      "    error(train)=2.32e-01, acc(train)=9.12e-01, error(valid)=4.78e-01, acc(valid)=8.51e-01, error(test)=5.21e-01, acc(test)=8.42e-01\n",
      "UP1: error(valid) at 23 = 4.87e-01 > at 18 = 4.56e-01\n",
      "Epoch 24: 181.7s to complete\n",
      "    error(train)=2.33e-01, acc(train)=9.13e-01, error(valid)=4.87e-01, acc(valid)=8.51e-01, error(test)=5.33e-01, acc(test)=8.40e-01\n",
      "UP1: error(valid) at 24 = 4.92e-01 > at 19 = 4.64e-01\n",
      "Epoch 25: 180.7s to complete\n",
      "    error(train)=2.28e-01, acc(train)=9.13e-01, error(valid)=4.92e-01, acc(valid)=8.50e-01, error(test)=5.32e-01, acc(test)=8.40e-01\n",
      "UP1: error(valid) at 25 = 4.91e-01 > at 20 = 4.71e-01\n",
      "UP2: error(valid) at 20 = 4.71e-01 > at 15 = 4.44e-01\n",
      "Epoch 26: 181.8s to complete\n",
      "    error(train)=2.19e-01, acc(train)=9.16e-01, error(valid)=4.91e-01, acc(valid)=8.52e-01, error(test)=5.42e-01, acc(test)=8.41e-01\n",
      "UP1: error(valid) at 26 = 5.00e-01 > at 21 = 4.93e-01\n",
      "UP2: error(valid) at 21 = 4.93e-01 > at 16 = 4.65e-01\n",
      "UP3: error(valid) at 16 = 4.65e-01 > at 11 = 4.49e-01\n",
      "EARLY STOPPING\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'keys' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-07751c05b590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mstats_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     earlyStopping=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c442787c7bf7>\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, test_data, num_epochs, stats_interval, notebook, earlyStopping)\u001b[0m\n\u001b[1;32m     52\u001b[0m         ax_1.plot(\n\u001b[1;32m     53\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             label=k)\n\u001b[1;32m     56\u001b[0m     \u001b[0max_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'keys' referenced before assignment"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAD8CAYAAABXV4w2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c90514400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "test_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    ReshapeLayer(output_shape=(1, 28, 28)),\n",
    "    ConvolutionalLayer(\n",
    "        num_input_channels=1,\n",
    "        num_output_channels=5,\n",
    "        input_dim_1=28,\n",
    "        input_dim_2=28,\n",
    "        kernel_dim_1=5,\n",
    "        kernel_dim_2=5,\n",
    "        padding=0,\n",
    "        stride=1),\n",
    "    MaxPoolingLayer(\n",
    "        num_input_channels=5, input_dim_1=24, input_dim_2=24, extent=2),\n",
    "    ReshapeLayer((12 * 12 * 5, )),\n",
    "    AffineLayer(12 * 12 * 5, hidden_dim, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropyLogSoftmaxError()\n",
    "learning_rule = AdamLearningRule()\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "trial1 = train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=False,\n",
    "    earlyStopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-22T00:26:25.114986Z",
     "start_time": "2017-11-22T00:26:25.110536Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(trial1, open('trial1.pkl'), protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-25T20:38:02.554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'targets']\n",
      "['inputs', 'targets']\n",
      "['inputs', 'targets']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2899.7s to complete\n",
      "    error(train)=5.77e-01, acc(train)=8.08e-01, error(valid)=5.95e-01, acc(valid)=8.03e-01, error(test)=6.35e-01, acc(test)=7.92e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-197c62772b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mstats_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     displayGraphs=False)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BASELINE_2layer_4DNN.{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MLP/MLP2k1718/mlp/helper.py\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, test_data, num_epochs, stats_interval, notebook, displayGraphs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Run the optimiser for 5 epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisplayGraphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MLP/MLP2k1718/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MLP/MLP2k1718/mlp/optimisers.py\u001b[0m in \u001b[0;36mdo_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mgrads_wrt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             grads_wrt_params = self.model.grads_wrt_params(\n\u001b[0;32m---> 71\u001b[0;31m                 activations, grads_wrt_outputs)\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_wrt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# train_progress_bar.update(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MLP/MLP2k1718/mlp/models.py\u001b[0m in \u001b[0;36mgrads_wrt_params\u001b[0;34m(self, activations, grads_wrt_outputs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mgrads_wrt_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_wrt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerWithParameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 grads_wrt_params += layer.grads_wrt_params(\n",
      "\u001b[0;32m~/Documents/MLP/MLP2k1718/mlp/layers.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, inputs, outputs, grads_wrt_outputs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvol2d_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_wrt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.helper import train_model_and_plot_stats\n",
    "from mlp.data_providers import EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016\n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "test_data = EMNISTDataProvider('test', batch_size=batch_size, rng=rng)\n",
    "\n",
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "from mlp.layers import *\n",
    "from mlp.errors import CrossEntropyLogSoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule, AdamLearningRule, RMSPropLearningRule\n",
    "from mlp.optimisers import Optimiser, EarlyStoppingOptimiser\n",
    "from mlp.penalty import *\n",
    "\n",
    "# setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "trial=1\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "test_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    ReshapeLayer(output_shape=(1, 28, 28)),\n",
    "    ConvolutionalLayer_NUMBA(\n",
    "        num_input_channels=1,\n",
    "        num_output_channels=5,\n",
    "        input_dim_1=28,\n",
    "        input_dim_2=28,\n",
    "        kernel_dim_1=5,\n",
    "        kernel_dim_2=5,\n",
    "        padding=0,\n",
    "        stride=1),\n",
    "    MaxPoolingLayer_NUMBA(\n",
    "        num_input_channels=5, input_dim_1=24, input_dim_2=24, extent=2),\n",
    "    ReshapeLayer(output_shape=(12 * 12 * 5,)),\n",
    "    ReluLayer(),\n",
    "    ReshapeLayer(output_shape=(5,12,12)),\n",
    "    ConvolutionalLayer_NUMBA(\n",
    "        num_input_channels=5,\n",
    "        num_output_channels=10,\n",
    "        input_dim_1=12,\n",
    "        input_dim_2=12,\n",
    "        kernel_dim_1=5,\n",
    "        kernel_dim_2=5,\n",
    "        padding=0,\n",
    "        stride=1),\n",
    "    MaxPoolingLayer_NUMBA(\n",
    "        num_input_channels=10, input_dim_1=8, input_dim_2=8, extent=2),\n",
    "    ReshapeLayer(output_shape=(4 * 4 * 10,)),\n",
    "    AffineLayer(4*4*10, 400, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(400, 400, weights_init, biases_init), # 4 hidden layers with 400 each!\n",
    "    ReluLayer(),\n",
    "    AffineLayer(400, 400, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(400, 400, weights_init, biases_init),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(400, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropyLogSoftmaxError()\n",
    "learning_rule = AdamLearningRule()\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "output = train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=True,\n",
    "    displayGraphs=False)\n",
    "pkl.dump(output, open('BASELINE_2layer_4DNN.{}.pkl'.format(trial), 'wb'), protocol=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 160,
   "position": {
    "height": "40px",
    "left": "845px",
    "right": "-11px",
    "top": "223px",
    "width": "529px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
