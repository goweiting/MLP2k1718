{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: 2,3 - RMSProp and Adam\n",
    "\n",
    "This notebook was primarily used for creating and testing of variations of Gradient Descent Learning Rule - RMSProp[1] and Adam[2].\n",
    "\n",
    "As noted in previous experiments, large number of deep layers results in large disturbances in convergence.\n",
    "\n",
    "\n",
    "[1] T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent\n",
    "magnitude. COURSERA: Neural Networks for Machine Learning, 4(2), 2012. URL https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.\n",
    "\n",
    "[2] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICML, 2015. URL\n",
    "https://arxiv.org/abs/1412.6980.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-12T11:12:05.367872Z",
     "start_time": "2017-11-12T11:12:05.345508Z"
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:45:29.326049Z",
     "start_time": "2017-11-13T21:45:29.219182Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:45:39.288858Z",
     "start_time": "2017-11-13T21:45:39.240396Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.learning_rules import *\n",
    "%aimport mlp.errors\n",
    "%aimport mlp.learning_rules\n",
    "%aimport mlp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:45:42.096398Z",
     "start_time": "2017-11-13T21:45:40.292611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'targets']\n",
      "['inputs', 'targets']\n",
      "['inputs', 'targets']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 10102016\n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "test_data = EMNISTDataProvider('test', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:45:42.212494Z",
     "start_time": "2017-11-13T21:45:42.143800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, ELULayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50 # TODO: CHANGED HERE FOR TESTING ONLY!\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:45:43.532811Z",
     "start_time": "2017-11-13T21:45:42.262258Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a single function to test a simple MLP with RELULayer:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(model,\n",
    "                               error,\n",
    "                               learning_rule,\n",
    "                               train_data,\n",
    "                               valid_data,\n",
    "                               test_data,\n",
    "                               num_epochs,\n",
    "                               stats_interval,\n",
    "                               notebook=True):\n",
    "\n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors = {'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model,\n",
    "        error,\n",
    "        learning_rule,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        test_data,\n",
    "        data_monitors,\n",
    "        notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(\n",
    "        num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(\n",
    "            np.arange(1, stats.shape[0]) * stats_interval,\n",
    "            stats[1:, keys[k]],\n",
    "            label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(\n",
    "            np.arange(1, stats.shape[0]) * stats_interval,\n",
    "            stats[1:, keys[k]],\n",
    "            label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Baseline\n",
    "\n",
    "Run an experiment using SGD learning rule for deep hidden layer (say 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:45:43.598198Z",
     "start_time": "2017-11-13T21:45:43.584127Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expt = {} # store all the experiments\n",
    "func = ELULayer()\n",
    "i = 2 # 10 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:47:18.894845Z",
     "start_time": "2017-11-13T21:47:18.871857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerModel(\n",
      "    AffineLayer(input_dim=784, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=47)\n",
      ") 10 layers\n"
     ]
    }
   ],
   "source": [
    "# Initialise the weights and biases:\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "input_layer = [\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init)\n",
    "]\n",
    "output_layer = [\n",
    "    func,\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "]\n",
    "each_hidden_layer = [\n",
    "    func,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init)\n",
    "]\n",
    "\n",
    "# create the MLP:\n",
    "model = MultipleLayerModel(input_layer + each_hidden_layer * i +\n",
    "                           output_layer)\n",
    "print(model, '{} layers'.format(i + 1))\n",
    "\n",
    "error = CrossEntropySoftmaxError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:46:06.011758Z",
     "start_time": "2017-11-13T21:45:45.186087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerModel(\n",
      "    AffineLayer(input_dim=784, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=100)\n",
      "    ELULayer\n",
      "    AffineLayer(input_dim=100, output_dim=47)\n",
      ") 10 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weiting/MLP2k1718/mlp/layers.py:833: RuntimeWarning: invalid value encountered in multiply\n",
      "  negative_gradients = negative_gradients * grads_wrt_outputs\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:834: RuntimeWarning: invalid value encountered in less\n",
      "  negative_gradients[negative_gradients < 1e-25] = 1e-25  # WITH CLIPPINGS\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:828: RuntimeWarning: invalid value encountered in greater\n",
      "  positive_gradients[positive_gradients > 1e25] = 1e25  # WITH CLIPPINGS\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:810: RuntimeWarning: invalid value encountered in maximum\n",
      "  positive_inputs = np.maximum(inputs, 0.)\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:811: RuntimeWarning: invalid value encountered in greater\n",
      "  positive_inputs[positive_inputs > 1e25] = 1e25  # WITH CLIPPINGS\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:814: RuntimeWarning: invalid value encountered in greater\n",
      "  negative_inputs[negative_inputs > 0] = 0.\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:816: RuntimeWarning: invalid value encountered in less\n",
      "  negative_inputs[negative_inputs < 1e-25] = 1e-25  # WITH CLIPPINGS\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:827: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  positive_gradients = (outputs >= 0) * grads_wrt_outputs\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:830: RuntimeWarning: invalid value encountered in less\n",
      "  outputs_to_use = (outputs < 0) * outputs\n",
      "/Users/weiting/MLP2k1718/mlp/layers.py:832: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  negative_gradients[outputs >= 0] = 0.\n",
      "Epoch 1: 7.0s to complete\n",
      "    error(train)=nan, acc(train)=2.13e-02, error(valid)=nan, acc(valid)=2.08e-02, error(test)=nan, acc(test)=2.15e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dda7137ea78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mstats_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     notebook=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-66244ab3ec3f>\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, test_data, num_epochs, stats_interval, notebook)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     stats, keys, run_time = optimiser.train(\n\u001b[0;32m---> 34\u001b[0;31m         num_epochs=num_epochs, stats_interval=stats_interval)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Plot the change in the validation and training set error over training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLP2k1718/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLP2k1718/mlp/optimisers.py\u001b[0m in \u001b[0;36mdo_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#     train_progress_bar.set_description(\"Epoch Progress\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mgrads_wrt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             grads_wrt_params = self.model.grads_wrt_params(\n",
      "\u001b[0;32m~/MLP2k1718/mlp/models.py\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs, evaluation)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mcurrent_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochastic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mcurrent_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLP2k1718/mlp/layers.py\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_wrt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data.reset()\n",
    "test_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "expt['SGD'] = train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum SGD\n",
    "\n",
    "Repeat the experiment with momentum SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-12T18:53:49.344308Z",
     "start_time": "2017-11-12T18:42:58.869194Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "test_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rule = MomentumLearningRule(learning_rate=0.01, mom_coeff=.9)\n",
    "\n",
    "expt['MomentumSGD'] = train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-12T11:04:15.439570Z",
     "start_time": "2017-11-12T11:04:15.057907Z"
    }
   },
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-12T19:05:38.849565Z",
     "start_time": "2017-11-12T19:03:47.859700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "test_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rule = RMSPropLearningRule(learning_rate=1e-3, beta=0.9, epsilon=1e-8)\n",
    "\n",
    "expt['RMSProp'] = train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-12T13:47:17.650559Z",
     "start_time": "2017-11-12T13:47:10.677806Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.reset()\n",
    "test_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rule = AdamLearningRule(learning_rate=1e-3, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=1e-8)\n",
    "\n",
    "expt['adam'] = train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
